{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Import Pypi</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io as scio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import SGD\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Import Model</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calc_map_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Basic Module</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModule, self).__init__()\n",
    "        self.module_name = str(type(self))\n",
    "\n",
    "    def load(self, path, use_gpu=False):\n",
    "        if not use_gpu:\n",
    "            self.load_state_dict(torch.load(path, map_location=lambda storage, loc: storage))\n",
    "        else:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "\n",
    "    def save(self, name=None):\n",
    "        if name is None:\n",
    "            prefix = self.module_name + '_'\n",
    "            name = time.strftime(prefix + '%m%d_%H:%M:%S.pth')\n",
    "        torch.save(self.state_dict(), 'checkpoints/' + name)\n",
    "        return name\n",
    "\n",
    "    def forward(self, *input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Image Module</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgModule(BasicModule):\n",
    "    def __init__(self, bit, pretrain_model=None):\n",
    "        super(ImgModule, self).__init__()\n",
    "        self.module_name = \"image_model\"\n",
    "        self.features = nn.Sequential(\n",
    "            # 0 conv1\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4),\n",
    "            # 1 relu1\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 2 norm1\n",
    "            nn.LocalResponseNorm(size=2, k=2),\n",
    "            # 3 pool1\n",
    "            nn.ZeroPad2d((0, 1, 0, 1)),\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), stride=2),\n",
    "            # 4 conv2\n",
    "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            # 5 relu2\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 6 norm2\n",
    "            nn.LocalResponseNorm(size=2, k=2),\n",
    "            # 7 pool2\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), stride=2),\n",
    "            # 8 conv3\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # 9 relu3\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 10 conv4\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # 11 relu4\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 12 conv5\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # 13 relu5\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 14 pool5\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0)),\n",
    "            # 15 full_conv6\n",
    "            nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=6),\n",
    "            # 16 relu6\n",
    "            nn.ReLU(inplace=True),\n",
    "            # 17 full_conv7\n",
    "            nn.Conv2d(in_channels=4096, out_channels=4096, kernel_size=1),\n",
    "            # 18 relu7\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # fc8\n",
    "        self.classifier = nn.Linear(in_features=4096, out_features=bit)\n",
    "        self.classifier.weight.data = torch.randn(bit, 4096) * 0.01\n",
    "        self.classifier.bias.data = torch.randn(bit) * 0.01\n",
    "        self.mean = torch.zeros(3, 224, 224)\n",
    "        if pretrain_model:\n",
    "            self._init(pretrain_model)\n",
    "\n",
    "    def _init(self, data):\n",
    "        weights = data['layers'][0]\n",
    "        self.mean = torch.from_numpy(data['normalization'][0][0][0].transpose()).type(torch.float)\n",
    "        for k, v in self.features.named_children():\n",
    "            k = int(k)\n",
    "            if isinstance(v, nn.Conv2d):\n",
    "                if k > 1:\n",
    "                    k -= 1\n",
    "                v.weight.data = torch.from_numpy(weights[k][0][0][0][0][0].transpose())\n",
    "                v.bias.data = torch.from_numpy(weights[k][0][0][0][0][1].reshape(-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.is_cuda:\n",
    "            x = x - self.mean.cuda()\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Text Module</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER1_NODE = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
    "        nn.init.normal_(m.bias.data, 0.0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxtModule(BasicModule):\n",
    "    def __init__(self, y_dim, bit):\n",
    "        super(TxtModule, self).__init__()\n",
    "        self.module_name = \"text_model\"\n",
    "\n",
    "        # full-conv layers\n",
    "        self.conv1 = nn.Conv2d(1, LAYER1_NODE, kernel_size=(y_dim, 1), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(LAYER1_NODE, bit, kernel_size=1, stride=(1, 1))\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Load Data</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    Data = h5py.File(path)\n",
    "    images = Data['IAll'][:]\n",
    "    labels = Data['LAll'][:]\n",
    "    tags = Data['TAll'][:]\n",
    "    images = images.transpose(3,2,0,1)\n",
    "    labels = labels.transpose(1,0)\n",
    "    tags = tags.transpose(1,0)\n",
    "    Data.close()\n",
    "    return images, tags, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_model(path):\n",
    "    return scio.loadmat(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Config</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultConfig(object):\n",
    "    load_img_path = None  # load model path\n",
    "    load_txt_path = None\n",
    "\n",
    "    # data parameters\n",
    "    data_path = './data/FLICKR-25K.mat'\n",
    "    pretrain_model_path = './data/imagenet-vgg-f.mat'\n",
    "    training_size = 10000\n",
    "    query_size = 2000\n",
    "    database_size = 18015\n",
    "    batch_size = 128\n",
    "\n",
    "    # hyper-parameters\n",
    "    max_epoch = 300\n",
    "    gamma = 1\n",
    "    eta = 1\n",
    "    bit = 64  # final binary code length\n",
    "    lr = 10 ** (-1.5)  # initial learning rate\n",
    "    use_gpu = True\n",
    "    valid = True\n",
    "    print_freq = 2  # print info every N epoch\n",
    "    result_dir = 'result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = DefaultConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Split Data</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images, tags, labels):\n",
    "    X = {}\n",
    "    X['query'] = images[0: opt.query_size]\n",
    "    X['train'] = images[opt.query_size: opt.training_size + opt.query_size]\n",
    "    X['retrieval'] = images[opt.query_size: opt.query_size + opt.database_size]\n",
    "\n",
    "    Y = {}\n",
    "    Y['query'] = tags[0: opt.query_size]\n",
    "    Y['train'] = tags[opt.query_size: opt.training_size + opt.query_size]\n",
    "    Y['retrieval'] = tags[opt.query_size: opt.query_size + opt.database_size]\n",
    "\n",
    "    L = {}\n",
    "    L['query'] = labels[0: opt.query_size]\n",
    "    L['train'] = labels[opt.query_size: opt.training_size + opt.query_size]\n",
    "    L['retrieval'] = labels[opt.query_size: opt.query_size + opt.database_size]\n",
    "\n",
    "    return X, Y, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Calc Neighbor</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_neighbor(label1, label2):\n",
    "    # calculate the similar matrix\n",
    "    if opt.use_gpu:\n",
    "        Sim = (label1.matmul(label2.transpose(0, 1)) > 0).type(torch.cuda.FloatTensor)\n",
    "    else:\n",
    "        Sim = (label1.matmul(label2.transpose(0, 1)) > 0).type(torch.FloatTensor)\n",
    "    return Sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Calc Loss</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(B, F, G, Sim, gamma, eta):\n",
    "    theta = torch.matmul(F, G.transpose(0, 1)) / 2\n",
    "    term1 = torch.sum(torch.log(1 + torch.exp(theta)) - Sim * theta)\n",
    "    term2 = torch.sum(torch.pow(B - F, 2) + torch.pow(B - G, 2))\n",
    "    term3 = torch.sum(torch.pow(F.sum(dim=0), 2) + torch.pow(G.sum(dim=0), 2))\n",
    "    loss = term1 + gamma * term2 + eta * term3\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Generate Image</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_code(img_model, X, bit):\n",
    "    batch_size = opt.batch_size\n",
    "    num_data = X.shape[0]\n",
    "    index = np.linspace(0, num_data - 1, num_data).astype(int)\n",
    "    B = torch.zeros(num_data, bit, dtype=torch.float)\n",
    "    if opt.use_gpu:\n",
    "        B = B.cuda()\n",
    "    for i in tqdm(range(num_data // batch_size + 1)):\n",
    "        ind = index[i * batch_size: min((i + 1) * batch_size, num_data)]\n",
    "        image = X[ind].type(torch.float)\n",
    "        if opt.use_gpu:\n",
    "            image = image.cuda()\n",
    "        cur_f = img_model(image)\n",
    "        B[ind, :] = cur_f.data\n",
    "    B = torch.sign(B)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Generate Text</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_code(txt_model, Y, bit):\n",
    "    batch_size = opt.batch_size\n",
    "    num_data = Y.shape[0]\n",
    "    index = np.linspace(0, num_data - 1, num_data).astype(int)\n",
    "    B = torch.zeros(num_data, bit, dtype=torch.float)\n",
    "    if opt.use_gpu:\n",
    "        B = B.cuda()\n",
    "    for i in tqdm(range(num_data // batch_size + 1)):\n",
    "        ind = index[i * batch_size: min((i + 1) * batch_size, num_data)]\n",
    "        text = Y[ind].unsqueeze(1).unsqueeze(-1).type(torch.float)\n",
    "        if opt.use_gpu:\n",
    "            text = text.cuda()\n",
    "        cur_g = txt_model(text)\n",
    "        B[ind, :] = cur_g.data\n",
    "    B = torch.sign(B)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Valid</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(img_model, txt_model, query_x, retrieval_x, query_y, retrieval_y, query_L, retrieval_L):\n",
    "    qBX = generate_image_code(img_model, query_x, opt.bit)\n",
    "    qBY = generate_text_code(txt_model, query_y, opt.bit)\n",
    "    rBX = generate_image_code(img_model, retrieval_x, opt.bit)\n",
    "    rBY = generate_text_code(txt_model, retrieval_y, opt.bit)\n",
    "\n",
    "    mapi2t = calc_map_k(qBX, rBY, query_L, retrieval_L)\n",
    "    mapt2i = calc_map_k(qBY, rBX, query_L, retrieval_L)\n",
    "    return mapi2t, mapt2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Train</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    images, tags, labels = load_data(opt.data_path)\n",
    "    pretrain_model = load_pretrain_model(opt.pretrain_model_path)\n",
    "    y_dim = tags.shape[1]\n",
    "\n",
    "    X, Y, L = split_data(images, tags, labels)\n",
    "    print('...loading and splitting data finish')\n",
    "\n",
    "    img_model = ImgModule(opt.bit, pretrain_model)\n",
    "    txt_model = TxtModule(y_dim, opt.bit)\n",
    "\n",
    "    if opt.use_gpu:\n",
    "        img_model = img_model.cuda()\n",
    "        txt_model = txt_model.cuda()\n",
    "\n",
    "    train_L = torch.from_numpy(L['train'])\n",
    "    train_x = torch.from_numpy(X['train'])\n",
    "    train_y = torch.from_numpy(Y['train'])\n",
    "\n",
    "    query_L = torch.from_numpy(L['query'])\n",
    "    query_x = torch.from_numpy(X['query'])\n",
    "    query_y = torch.from_numpy(Y['query'])\n",
    "\n",
    "    retrieval_L = torch.from_numpy(L['retrieval'])\n",
    "    retrieval_x = torch.from_numpy(X['retrieval'])\n",
    "    retrieval_y = torch.from_numpy(Y['retrieval'])\n",
    "\n",
    "    num_train = train_x.shape[0]\n",
    "\n",
    "    F_buffer = torch.randn(num_train, opt.bit)\n",
    "    G_buffer = torch.randn(num_train, opt.bit)\n",
    "\n",
    "    if opt.use_gpu:\n",
    "        train_L = train_L.cuda()\n",
    "        F_buffer = F_buffer.cuda()\n",
    "        G_buffer = G_buffer.cuda()\n",
    "\n",
    "    Sim = calc_neighbor(train_L, train_L)\n",
    "    B = torch.sign(F_buffer + G_buffer)\n",
    "\n",
    "    batch_size = opt.batch_size\n",
    "\n",
    "    lr = opt.lr\n",
    "    optimizer_img = SGD(img_model.parameters(), lr=lr)\n",
    "    optimizer_txt = SGD(txt_model.parameters(), lr=lr)\n",
    "\n",
    "    learning_rate = np.linspace(opt.lr, np.power(10, -6.), opt.max_epoch + 1)\n",
    "    result = {\n",
    "        'loss': []\n",
    "    }\n",
    "\n",
    "    ones = torch.ones(batch_size, 1)\n",
    "    ones_ = torch.ones(num_train - batch_size, 1)\n",
    "    unupdated_size = num_train - batch_size\n",
    "\n",
    "    max_mapi2t = max_mapt2i = 0.\n",
    "\n",
    "    for epoch in range(opt.max_epoch):\n",
    "        # train image net\n",
    "        for i in tqdm(range(num_train // batch_size)):\n",
    "            index = np.random.permutation(num_train)\n",
    "            ind = index[0: batch_size]\n",
    "            unupdated_ind = np.setdiff1d(range(num_train), ind)\n",
    "\n",
    "            sample_L = Variable(train_L[ind, :])\n",
    "            image = Variable(train_x[ind].type(torch.float))\n",
    "            if opt.use_gpu:\n",
    "                image = image.cuda()\n",
    "                sample_L = sample_L.cuda()\n",
    "                ones = ones.cuda()\n",
    "                ones_ = ones_.cuda()\n",
    "\n",
    "            # similar matrix size: (batch_size, num_train)\n",
    "            S = calc_neighbor(sample_L, train_L)  # S: (batch_size, num_train)\n",
    "            cur_f = img_model(image)  # cur_f: (batch_size, bit)\n",
    "            F_buffer[ind, :] = cur_f.data\n",
    "            F = Variable(F_buffer)\n",
    "            G = Variable(G_buffer)\n",
    "\n",
    "            theta_x = 1.0 / 2 * torch.matmul(cur_f, G.t())\n",
    "            logloss_x = -torch.sum(S * theta_x - torch.log(1.0 + torch.exp(theta_x)))\n",
    "            quantization_x = torch.sum(torch.pow(B[ind, :] - cur_f, 2))\n",
    "            balance_x = torch.sum(torch.pow(cur_f.t().mm(ones) + F[unupdated_ind].t().mm(ones_), 2))\n",
    "            loss_x = logloss_x + opt.gamma * quantization_x + opt.eta * balance_x\n",
    "            loss_x /= (batch_size * num_train)\n",
    "\n",
    "            optimizer_img.zero_grad()\n",
    "            loss_x.backward()\n",
    "            optimizer_img.step()\n",
    "\n",
    "        # train txt net\n",
    "        for i in tqdm(range(num_train // batch_size)):\n",
    "            index = np.random.permutation(num_train)\n",
    "            ind = index[0: batch_size]\n",
    "            unupdated_ind = np.setdiff1d(range(num_train), ind)\n",
    "\n",
    "            sample_L = Variable(train_L[ind, :])\n",
    "            text = train_y[ind, :].unsqueeze(1).unsqueeze(-1).type(torch.float)\n",
    "            text = Variable(text)\n",
    "            if opt.use_gpu:\n",
    "                text = text.cuda()\n",
    "                sample_L = sample_L.cuda()\n",
    "\n",
    "            # similar matrix size: (batch_size, num_train)\n",
    "            S = calc_neighbor(sample_L, train_L)  # S: (batch_size, num_train)\n",
    "            cur_g = txt_model(text)  # cur_f: (batch_size, bit)\n",
    "            G_buffer[ind, :] = cur_g.data\n",
    "            F = Variable(F_buffer)\n",
    "            G = Variable(G_buffer)\n",
    "\n",
    "            # calculate loss\n",
    "            # theta_y: (batch_size, num_train)\n",
    "            theta_y = 1.0 / 2 * torch.matmul(cur_g, F.t())\n",
    "            logloss_y = -torch.sum(S * theta_y - torch.log(1.0 + torch.exp(theta_y)))\n",
    "            quantization_y = torch.sum(torch.pow(B[ind, :] - cur_g, 2))\n",
    "            balance_y = torch.sum(torch.pow(cur_g.t().mm(ones) + G[unupdated_ind].t().mm(ones_), 2))\n",
    "            loss_y = logloss_y + opt.gamma * quantization_y + opt.eta * balance_y\n",
    "            loss_y /= (num_train * batch_size)\n",
    "\n",
    "            optimizer_txt.zero_grad()\n",
    "            loss_y.backward()\n",
    "            optimizer_txt.step()\n",
    "\n",
    "        # update B\n",
    "        B = torch.sign(F_buffer + G_buffer)\n",
    "\n",
    "        # calculate total loss\n",
    "        loss = calc_loss(B, F, G, Variable(Sim), opt.gamma, opt.eta)\n",
    "\n",
    "        print('...epoch: %3d, loss: %3.3f, lr: %f' % (epoch + 1, loss.data, lr))\n",
    "        result['loss'].append(float(loss.data))\n",
    "\n",
    "        if opt.valid:\n",
    "            mapi2t, mapt2i = valid(img_model, txt_model, query_x, retrieval_x, query_y, retrieval_y,\n",
    "                                   query_L, retrieval_L)\n",
    "            print('...epoch: %3d, valid MAP: MAP(i->t): %3.4f, MAP(t->i): %3.4f' % (epoch + 1, mapi2t, mapt2i))\n",
    "            if mapt2i >= max_mapt2i and mapi2t >= max_mapi2t:\n",
    "                max_mapi2t = mapi2t\n",
    "                max_mapt2i = mapt2i\n",
    "                img_model.save(img_model.module_name + '.pth')\n",
    "                txt_model.save(txt_model.module_name + '.pth')\n",
    "\n",
    "        lr = learning_rate[epoch + 1]\n",
    "\n",
    "        # set learning rate\n",
    "        for param in optimizer_img.param_groups:\n",
    "            param['lr'] = lr\n",
    "        for param in optimizer_txt.param_groups:\n",
    "            param['lr'] = lr\n",
    "\n",
    "    print('...training procedure finish')\n",
    "    if opt.valid:\n",
    "        result['mapi2t'] = max_mapi2t\n",
    "        result['mapt2i'] = max_mapt2i\n",
    "    else:\n",
    "        mapi2t, mapt2i = valid(img_model, txt_model, query_x, retrieval_x, query_y, retrieval_y, query_L, retrieval_L)\n",
    "        result['mapi2t'] = mapi2t\n",
    "        result['mapt2i'] = mapt2i\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><p align=\"center\">Test</p></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    images, tags, labels = load_data(opt.data_path)\n",
    "    y_dim = tags.shape[1]\n",
    "\n",
    "    X, Y, L = split_data(images, tags, labels)\n",
    "    print('...loading and splitting data finish')\n",
    "\n",
    "    img_model = ImgModule(opt.bit)\n",
    "    txt_model = TxtModule(y_dim, opt.bit)\n",
    "\n",
    "    if opt.load_img_path:\n",
    "        img_model.load(opt.load_img_path)\n",
    "\n",
    "    if opt.load_txt_path:\n",
    "        txt_model.load(opt.load_txt_path)\n",
    "\n",
    "    if opt.use_gpu:\n",
    "        img_model = img_model.cuda()\n",
    "        txt_model = txt_model.cuda()\n",
    "\n",
    "    query_L = torch.from_numpy(L['query'])\n",
    "    query_x = torch.from_numpy(X['query'])\n",
    "    query_y = torch.from_numpy(Y['query'])\n",
    "\n",
    "    retrieval_L = torch.from_numpy(L['retrieval'])\n",
    "    retrieval_x = torch.from_numpy(X['retrieval'])\n",
    "    retrieval_y = torch.from_numpy(Y['retrieval'])\n",
    "\n",
    "    qBX = generate_image_code(img_model, query_x, opt.bit)\n",
    "    qBY = generate_text_code(txt_model, query_y, opt.bit)\n",
    "    rBX = generate_image_code(img_model, retrieval_x, opt.bit)\n",
    "    rBY = generate_text_code(txt_model, retrieval_y, opt.bit)\n",
    "\n",
    "    if opt.use_gpu:\n",
    "        query_L = query_L.cuda()\n",
    "        retrieval_L = retrieval_L.cuda()\n",
    "\n",
    "    mapi2t = calc_map_k(qBX, rBY, query_L, retrieval_L)\n",
    "    mapt2i = calc_map_k(qBY, rBX, query_L, retrieval_L)\n",
    "    return mapi2t, mapt2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = train()\n",
    "print('   max MAP: MAP(i->t): %3.4f, MAP(t->i): %3.4f' % (result['mapi2t'], result['mapt2i']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapi2t, mapt2i = test()\n",
    "print('...test MAP: MAP(i->t): %3.3f, MAP(t->i): %3.3f' % (mapi2t, mapt2i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
